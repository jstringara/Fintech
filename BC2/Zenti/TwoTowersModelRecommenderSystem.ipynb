{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# The evolution of collaborative filtering: the Two Tower Model\n",
        "\n",
        "In a sense, it is an evolution of the SVD. The idea is more or less the same: finding latent variables.\n",
        "\n",
        "A two tower model is a type of recommender system that uses two separate neural networks, often called \"towers\", to make recommendations.\n",
        "The users and items are represented as N-dimensional embedding vectors, these are learned by the model such that the similarity score between a user and item representation is higher for items with which the user has interacted. The name two towers is derived from the fact that there are 2 towers one for learning the encoding of the users and the other for learning the encodings of the items.\n",
        "\n",
        "* The first tower is a **feature encoder** that takes the explanatory variables and encodes them into a fixed-length vector representation - basically a vector of latent variables spotted in a non-linear fashion. This vector representation is then passed to the second tower. See: https://blog.tensorflow.org/2020/09/introducing-tensorflow-recommenders.html\n",
        "\n",
        "* The second tower is a **ranking model** that uses the encoded features to score and rank the potential recommendations, i.e., it links users and products, based on the given examples.\n",
        "\n",
        "\n",
        "Finally, we can use the trained model to make recommendations on new, unseen data. This will involve passing the explanatory variables through the feature encoder tower to get the encoded features, and then passing those features to the ranking model tower to score and rank the recommendations.\n",
        "\n",
        "In what follows there is just one example of how a two tower model can be used for recommendation (there are many different variations and ways to implement this type of model, and the specific details will depend on the particular problem and data at hand). This a toy example to illustrate the basic steps involved in building a recommender system using a neural network - you can start from here and try to work on the theme yourself."
      ],
      "metadata": {
        "id": "S9TOPTmJLlnr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Example\n",
        "We first load the libraries, create some *synthetic data* and split it into training and testing sets, as usual.\n",
        "Let's assume we have 5 products to recommend, and 10 explanatory variables. \n",
        "\n",
        "**HINTS**: Try with different synthetic data (try more sensible probability distributions), or with real data."
      ],
      "metadata": {
        "id": "a175aZ_DLnq6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Dense, Embedding, Flatten, Concatenate\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(0)\n",
        "\n",
        "# Define the number of recommendations (say 5, purely as an example, but you can change) and explanatory variables (say 10)\n",
        "num_recommendations = 5\n",
        "num_explanatory_variables = 10\n",
        "\n",
        "# Generate simulated data\n",
        "X = np.random.rand(1000, num_explanatory_variables)\n",
        "y = np.random.randint(0, 2, (1000, num_recommendations))\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "\n"
      ],
      "metadata": {
        "id": "r1J1ZiHVLKDG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we define **model architecture**, in its most classic form:\n",
        "* The model consists of two \"towers\".\n",
        "* Each tower has two hidden layers, and a merged layer that concatenates the output of the two towers and feeds it into two more hidden layers before producing the final output.\n",
        "* More in detail, the input layer receives the explanatory variables as input.\n",
        "* The embedding layer converts the input variables into a lower-dimensional representation (using an embedding matrix).\n",
        "* The flatten layer flattens the output of the embedding layer into a 1D vector.\n",
        "* The two towers each consist of two hidden layers (with ReLU activation), as described above, that transform the flattened embedding vectors into intermediate representations that are specific to each tower.\n",
        "* The output layers of each tower produce probability distributions over the five possible products to recommend (using the softmax activation function).\n",
        "* The concatenated output of the two towers is fed into two more hidden layers (with ReLU activation) that combine the two towers' outputs into a single, final representation.\n",
        "* The output layer produces a probability distribution over the N=5 products to recommend (again, using the softmax activation function).\n",
        "* We compile the model, preparing it for training, using the Adam optimizer and categorical cross-entropy loss function - quite standard."
      ],
      "metadata": {
        "id": "yZgOPzkmL6aH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define model architecture\n",
        "input_dim = X_train.shape[1]\n",
        "output_dim = y_train.shape[1]\n",
        "\n",
        "input_layer = Input(shape=(input_dim,))\n",
        "embedding_layer = Embedding(input_dim=100, output_dim=10)(input_layer)\n",
        "flatten_layer = Flatten()(embedding_layer)\n",
        "\n",
        "tower1_layer1 = Dense(64, activation='relu')(flatten_layer)\n",
        "tower1_layer2 = Dense(32, activation='relu')(tower1_layer1)\n",
        "tower1_output = Dense(output_dim, activation='softmax', name='tower1_output')(tower1_layer2)\n",
        "\n",
        "tower2_layer1 = Dense(64, activation='relu')(flatten_layer)\n",
        "tower2_layer2 = Dense(32, activation='relu')(tower2_layer1)\n",
        "tower2_output = Dense(output_dim, activation='softmax', name='tower2_output')(tower2_layer2)\n",
        "\n",
        "concat_layer = Concatenate()([tower1_layer2, tower2_layer2])\n",
        "merged_layer1 = Dense(32, activation='relu')(concat_layer)\n",
        "merged_output = Dense(output_dim, activation='softmax', name='merged_output')(merged_layer1)\n",
        "\n",
        "# Define the model\n",
        "model = Model(inputs=[input_layer], outputs=[tower1_output, tower2_output, merged_output])\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "Qv86hHeuL7Ag"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Now we **train and evaluate** the model.\n",
        "\n",
        "And, finally, we do our **predictions**."
      ],
      "metadata": {
        "id": "JvkFaNuqrTYY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "model.fit(X_train, [y_train, y_train, y_train], epochs=10, batch_size=32)\n",
        "\n",
        "# Evaluate the model\n",
        "loss, tower1_loss, tower2_loss, merged_loss, tower1_acc, tower2_acc, merged_acc = model.evaluate(X_test, [y_test, y_test, y_test], verbose=0)\n",
        "print('Overall Loss:', loss)\n",
        "print('Tower 1 Loss:', tower1_loss)\n",
        "print('Tower 2 Loss:', tower2_loss)\n",
        "print('Merged Loss:', merged_loss)\n",
        "print('Tower 1 Accuracy:', tower1_acc)\n",
        "print('Tower 2 Accuracy:', tower2_acc)\n",
        "print('Merged Accuracy:', merged_acc)\n",
        "\n",
        "# Make predictions on test set\n",
        "predictions = model.predict(X_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rgNq7fRbrPDT",
        "outputId": "3a90c850-4821-40bf-bd00-51ce4450c611"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "25/25 [==============================] - 3s 5ms/step - loss: 12.1660 - tower1_output_loss: 4.0463 - tower2_output_loss: 4.0686 - merged_output_loss: 4.0511 - tower1_output_accuracy: 0.2288 - tower2_output_accuracy: 0.5163 - merged_output_accuracy: 0.0425\n",
            "Epoch 2/10\n",
            "25/25 [==============================] - 0s 5ms/step - loss: 12.6974 - tower1_output_loss: 4.0580 - tower2_output_loss: 4.4997 - merged_output_loss: 4.1397 - tower1_output_accuracy: 0.1238 - tower2_output_accuracy: 0.5163 - merged_output_accuracy: 0.0288\n",
            "Epoch 3/10\n",
            "25/25 [==============================] - 0s 4ms/step - loss: 16.7959 - tower1_output_loss: 4.3267 - tower2_output_loss: 7.1200 - merged_output_loss: 5.3492 - tower1_output_accuracy: 0.1238 - tower2_output_accuracy: 0.4825 - merged_output_accuracy: 0.0300\n",
            "Epoch 4/10\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 25.5994 - tower1_output_loss: 5.3386 - tower2_output_loss: 11.6376 - merged_output_loss: 8.6231 - tower1_output_accuracy: 0.1238 - tower2_output_accuracy: 0.3000 - merged_output_accuracy: 0.0437\n",
            "Epoch 5/10\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 30.8326 - tower1_output_loss: 5.3701 - tower2_output_loss: 12.4593 - merged_output_loss: 13.0032 - tower1_output_accuracy: 0.1238 - tower2_output_accuracy: 0.2537 - merged_output_accuracy: 0.0913\n",
            "Epoch 6/10\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 32.8087 - tower1_output_loss: 4.6362 - tower2_output_loss: 18.1362 - merged_output_loss: 10.0362 - tower1_output_accuracy: 0.1238 - tower2_output_accuracy: 0.1450 - merged_output_accuracy: 0.0988\n",
            "Epoch 7/10\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 37.2496 - tower1_output_loss: 4.4291 - tower2_output_loss: 27.4199 - merged_output_loss: 5.4006 - tower1_output_accuracy: 0.0988 - tower2_output_accuracy: 0.2338 - merged_output_accuracy: 0.2013\n",
            "Epoch 8/10\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 55.1807 - tower1_output_loss: 5.0126 - tower2_output_loss: 38.0243 - merged_output_loss: 12.1439 - tower1_output_accuracy: 0.1412 - tower2_output_accuracy: 0.2087 - merged_output_accuracy: 0.2300\n",
            "Epoch 9/10\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 90.2003 - tower1_output_loss: 5.2470 - tower2_output_loss: 49.7228 - merged_output_loss: 35.2305 - tower1_output_accuracy: 0.1400 - tower2_output_accuracy: 0.2575 - merged_output_accuracy: 0.2188\n",
            "Epoch 10/10\n",
            "25/25 [==============================] - 0s 3ms/step - loss: 113.9655 - tower1_output_loss: 5.7001 - tower2_output_loss: 57.6239 - merged_output_loss: 50.6416 - tower1_output_accuracy: 0.2100 - tower2_output_accuracy: 0.2075 - merged_output_accuracy: 0.2150\n",
            "Overall Loss: 143.5790557861328\n",
            "Tower 1 Loss: 5.915769100189209\n",
            "Tower 2 Loss: 53.61212921142578\n",
            "Merged Loss: 84.0511703491211\n",
            "Tower 1 Accuracy: 0.15000000596046448\n",
            "Tower 2 Accuracy: 0.15000000596046448\n",
            "Merged Accuracy: 0.054999999701976776\n",
            "7/7 [==============================] - 0s 3ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "And now we can use the trained model to **make recommendations on some new data**."
      ],
      "metadata": {
        "id": "udCO_F_16s-w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the trained model to make recommendations on some new data (just an example)\n",
        "X_new = np.random.rand(100, num_explanatory_variables)\n",
        "predicted_scores = model.predict(X_new)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NLjsC7rhr0XR",
        "outputId": "4de836af-22d8-474e-f365-26392f09adc0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4/4 [==============================] - 0s 3ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Some remarks\n",
        "There are some advantages of a two-tower model compared to a standard SVD model for recommender systems:\n",
        "\n",
        "* Handling of non-linear relationships: A two-tower model can handle non-linear relationships between the input variables and the recommended products, whereas an SVD model assumes a linear relationship. It models the complex relationships between variables and recommended products - which might end in overfitting, BTW.\n",
        "\n",
        "* Incorporating additional features: a two-tower model can incorporate additional features beyond user-item interactions, such as user and item attributes or contextual information. This allows the model to capture more complex relationships between users, items, and the environment in which they interact.\n",
        "\n",
        "* Scalability: a two-tower model can be more scalable than SVD models because it can be trained using mini-batch gradient descent, which can handle larger datasets more efficiently. SVD models require computing the full matrix factorization, which can be computationally expensive and memory-intensive."
      ],
      "metadata": {
        "id": "ApBTcbYN-cf2"
      }
    }
  ]
}